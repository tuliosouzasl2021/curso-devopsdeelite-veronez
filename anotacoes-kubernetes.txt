# ANOTAÇÕES KUBERNETES # 

Como você pode ver na primeira aula o devops representa um fluxo continuo de produção de software, então tão importante quando saber operar uma ferramenta especifica, é saber conectala no fluxo com as ferramentas. Inclusive essa é uma das minhas prioridades no devopspro, dar a clareza na integração das ferramentas.


# CHEGOU A HORA DE CONHECER O KUBERNETES #

Lembra que eu te falei na aula 1 que existe alguns problemas relacionadas a containers como por exemplo: escalabilidade, reseliência que o docker não resolve? então, o kubernetes vem justamente para trazer essas soluções orquestrado os containers e automatizando os processos.

Com ele é possivel garantir, resiliência e escalabilidade nas minhas aplicações, ou seja, eu consigo garantir que a minha aplicação não caia, alem de conseguir escalar e adequar a aplicação para um aumento de carga, alem disso eu consigo fazer o "BALANCIAMENTO DE CARGA" entre as replicas no momento em que escalo a aplicação, com isso eu evito a sobre carga de uma das replicas distribuindo a carga entre todas elas, e tem mais, eu não preciso ficar procurando um processo expecifico pois o "SERVICE DISCOVERY" disponivel no kubernetes  

# COMANDOS KUBERNETES #

k3d cluster create
k3d cluster create meucluster
k3d cluster list 
k3d cluster create meucluster2 --no-lb
k3d cluster delete
k3d cluster create meucluster --servers 3 --agents 3

kubectl get nodes


# POD #

Menor elemento do kubernetes é nele que executo os meus containers.

-> COMO CRIAR MEU POD NO CLUSTER KUBERNETES 

OBS: PARA CRIAR QUALQUER OBJETO NO CLUSTER KUBERNETES EU PRECISO CRIAR UM ARQUIVO DE MANIFESTO, OQ SERIA ESSE ARQUIVO DE MANIFESTO, É UM ARQUIVO YAML COM TODOS OS DADOS QUE EU QUERO CRIAR ALI PARA ESSE OBJETO NO CLUSTER KUBERNETES, VOU COLOCAR TODOS OS METADADOS e TODAS ESPECIFICAÇÃO QUE EU QUERO CRIAR E VOU APLICAR ISSO NO CLUSTER KUBERNETES ELE VAI LA E CRIA PARA MIM.

pod.yaml
apiVersion:
kind:
metadata:
spec:

ESSES CAMPOS SÃO OS PRINCIPAIS EM QUALQUER MANIFESTO NO CLUSTER DE KUBERNETES.

No apiVersion eu vou expecificar o grupo de apis que eu vou utilizar para esse objeto, como eu faço para saber o grupo, comando abaixo:
kubectl api-resources, esse cara vai nos mostrar todos os objetos que eu tenho disponiveis nesse cluster de kubernetes e ele fala também qual é o apiversion.
Por exemplo, vamos criar um pod e la mostra o apiversion dele.

pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: meupod
spec:
 containers:
 - name: site
   image: fabricioveronez/web-page:blue
   ports:
   - containerPort: 80

kubectl apply -f pod.yaml    --- > vou criar ou atualizar
kubectl create -f pod.yaml   --- > vou só criar

kubectl get pods	--- > lista os pods, o "kubectl get" sempre lista um objeto do cluster de kubernetes.
kubectl describe pod meupod	-- > ver mais detalhes do seu pod, o meupod é o nome que vc deu para ele no arquivo yaml.
kubectl describe pod/meupod	-- > ver mais detalhes do seu pod, o meupod é o nome que vc deu para ele no arquivo yaml.
kubectl port-forward pod/meupod 8080:80		-- > ele vai fazer o portbind igual faz no docker.
kubectl delete pod meupod  	-- > deleta o pod informado.
kubectl delete -f pod.yaml	-- > deletar o pod apartir do arquivo yaml de manifesto de criação.

# labels  e selector #

Labels vc cria para poder dar um apelido para o pod, replicaset e etc....
no caso abaixo está atribuida ao pod: meupod-azul e meupod-verde, nesse caso estamos criando 2 pods em um arquivo yaml só, separados pelo o caracter: ---


Arquivo pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: meupod-azul
 labels:
  cor: azul
spec:
 containers:
 - name: site
   image: fabricioveronez/web-page:blue
   ports:
   - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
 name: meupod-verde
 labels:
  cor: verde
spec:
 containers:
 - name: site
   image: fabricioveronez/web-page:green
   ports:
   - containerPort: 80

kubectl get pods -l cor=verde
kubectl get pods -l cor=azul

kubectl delete pods -l cor=azul
kubectl delete pods -l cor=verde


o replicaset é um controlador responsavel por gerenciar os meus pods e sempre manter ele de pe.

criando o replicaset para configurar a quantidade de replicas.

kubectl api-resources 	-- > validar a versão do objeto.

arquivo: replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: meureplicaset
spec:
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:blue
     ports:
     - containerPort: 80

kubectl apply -f replicaset.yaml

kubectl get pods 	-- > o nome do pode é criado randomicamente, com o nome do meu replicaset que nesse exemplo é: "meureplicaset-xxxxx"

kubectl get replicaset 	-- > listagem dos meus replicasets

kubctl describe replicaset meureplicaset 	-- > detalhes do meu replicaset


arquivo: replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: meureplicaset
spec:
 replicas: 5
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:blue
     ports:
     - containerPort: 80

colocando 5 replicas.

kubectl describe replicaset meureplicaset

kubectl delete pod meureplicaset-xxxxx

arquivo: replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: meureplicaset
spec:
 replicas: 5
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:green
     ports:
     - containerPort: 80

mudando da pagina blue para green, porém o pod só vai aceitar essa configuração quando eu matar ele e o replicaset criar um novo, sendo assim ele gera um pod novo com essa nova imagem,
nesse caso nos alteramos a imagem do container, com tudo isso existe o deployment que gerencia os replicasets..

kubectl get pods

kubectl get replicaset meureplicaset

kubectl describe replicaset meureplicaset 	-- > sendo assim ele mudou a imagem para o green no replicaset, porém ele não mudou nos pods, porque ele só vai mudar quando o pod morrer ou alguem força o pod que ainda está com a config antiga morrer, sendo assim o replicaset não faz esse gerenciamento e quem toma conta disso é o deployment.

kubectl get pods

kubectl describe pod meureplicaset-xxxxx	-- > ele está com a imagem blue ainda.

kubectl delete pod meureplicaset-xxxxx		-- > quando ocorrer o delete desse pod o replicaset vai gerar um novo pod, pois nas configurações do replicaset ele precisa ter 5 replicas sempre, nesse caso o pod que subir, vai subir com a imagem green.

kubectl describe pod meureplicaset-yyyyy 	-- > pod novo com a imagem green.

# deployment # -> TEMPO 1:22:00

O deployment é um controlador que fica acima do replicaset gerenciando as versões do replicaset.

O replicaset garante que a quantidade desejada de replicas e a que vai estar em execução no cluster de kubernetes ou pelo menos ele tenta fazer isso, já o deploymente ele vai gerenciar as versões do replicaset, porque quando é alterado alguma especificação no pod que está rodando naquele momento ele vai gerar um novo replicaset com a especificação alterada, já o antigo se mantem la do jeito que tava, só que a diferença é que os pods que vão estar em execução não vão ser mais gerenciado por ele vão ser gerenciado progressivamente pelo novo replicaset, então aos poucos os pods vão sendo criado no novo replicaset e vão sendo elimitados ali do antigo replicaset, porém o antigo replicaset fica la, porque caso ocorra um problema nessa nova versão eu consigo fazer o roolback no antigo replicaset.


arquivo: deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: meudeployment
spec:
 replicas: 1
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:blue
     ports:
     - containerPort: 80

kubectl apply -f deployment.yaml

kubectl get pods

kubectl get replicaset 

arquivo: deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: meudeployment
spec:
 replicas: 5 
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:blue
     ports:
     - containerPort: 80

kubectl apply -f deployment.yaml

kubectl get pods

arquivo: deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: meudeployment
spec:
 replicas: 5 
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:green
     ports:
     - containerPort: 80

kubectl apply -f deployment.yaml

ele está progressivamente mudando os pods para a imagem green e tirando os pods do antigo replicaset, porém o antigo replicaset ainda fica criado, pois se der merda conseguimos voltar com o comando abaixo:

kubectl rollout undo deployment meudeployment

kubectl get replicaset 

kubectl get pod meudeployment-xxxxxx-xxxxxx

kubectl port-forward pod/meuployment-xxxxxx-xxxxx 8080:80 


COmo estamos trabalhando com varias replicas precisamos de um ponto unico de comunicação para encontrar o endereçamento dos meus pods e também fazer o balaceamento de carga entre eles, porque n adianta replicar e ter os pods se eu estou mandando requisição apenas para um pod.

Agora vamos aprender a fazer isso utilizando o services.



# SERVICES #

Ele é responsalve por expor os pods.


O service é o elemento que vai gerar a comunicação entre os pods, não só a comunicação entre os pods mas também externamento no cluster kubernetes, então toda vez que voc~e tiver uma aplicação que está rodando em um pod e vai se comunica com outro pods que vai ser uma outra aplicação o cenario de microserviço está ai é isso que vai acontcer, você vai fazer a comunicação de um pod para outro através do services, existem alguns tipos de pods, segue abaixo alguns deles:

TIPO DE SERVICE

* ClusterIP
-> É justamento o service que vai se comunicar internamente no cluster de kubernetes, ele tem um ip porém vc não vai utilizar o ip não é utilizado ip quando você trabalha em container, mas ele é o ponto de comunicação dos pods internamente no cluster de kubernetes, não é utilizado o ClusterIP para comunicação externa só interna.


* NodePort
-> Já o tipo nodeport é diferente, você consegue expor seus serviços externamente, então o usuário que está fora do cluster consegue acessar a aplicação, porém o acesso vai ser apartir de um dos nós do cluster de kubernetes, então para ter acesso o service e para acessar os pods você vai utilizar o ip de um dos nós do cluster.
É importante dizer que o Service do tipo NodePort ele elege uma porta entre o range de 30000 a 32767 para ser a porta de comunicação.


* LOadBalancer
-> Já o tipo LoadBalancer ele cria um ip publico para ser utilizado como acesso ao service, dessa forma você vai consumir um serviço do seu provedor de nuvem, então o loadbalancer na maioria dos casos é utilizado em casos que voc~e está consumido um kubernetes de um cloud provider (aws, digitalocean etc..).


Criando um services 


arquivo: deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: meudeployment
spec:
 replicas: 5 
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:green
     ports:
     - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
 name: web-page
spec:
 selector:
  app: web
 ports:
  - port: 80
    protocol: TCP
 type: NodePort

kubectl apply -f deployment.yaml

kubectl get services or kubectl get svc

kubect get all 

kubectl port-forward service/web-page 8080:80 

aumentar para 10 replicas

kubectl apply -f deployment.yaml

mudar a imagem para blue

kubectl apply -f deployment.yaml

manter o port forward não é uma boa porque toda hora que você mudar a porta do node vai mudar tbm, então vamos expor uma porta em um dos nós de kubernetes.

atráves do k3d eu consigo fazer um port bind.

k3d cluster delete meucluster

k3d cluster create meucluster -p "80:30000@loadbalancer"

kubectl get pods

kubeclt get nodes

docker container ls

então ele fez o portbind para a 30000.

kubectl apply -f deployment.yaml

kubectl get svc 

nesse caso ele subiu uma porta diferente de 30000, porém existe um objeto no service que é utilziado apenas no nodePort que é para você definir a porta dele.

arquivo: deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: meudeployment
spec:
 replicas: 5 
 selector:
  matchLabels:
   app: web
 template:
  metadata:
   labels:
    app: web
  spec:
   containers:
   - name: site
     image: fabricioveronez/web-page:green
     ports:
     - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
 name: web-page
spec:
 selector:
  app: web
 ports:
  - port: 80
    protocol: TCP
 type: NodePort
 nodePort: 30000

kubectl apply -f deployment.yaml

kubectl get svc 

agora ele vai utilizar a porta 30000 e agora vai funcionar. 



Projeto kube-news

k3d cluster delete meucluster

k3d cluster create meucluster -p "80:30000@loadbalancer"

fazer o fork do projeto github.com/KubeDev/kube-news

git clone git@github.com:tuliosouzasl2021/kube-news.git .


passo a passo para fazer o deploy da aplicação.

vim Dockerfile
FROM node:18.11.0
WORKDIR /app
COPY package*.json ./
RUN npm install -y
COPY . .
EXPOSE 8080
CMD ["node", "server.js"]

docker build -t tuliosouzasl2021/kube-news:v1 . or docker build -t tuliosouzasl2021/kube-news:v1 -f Dockerfile

docker image ls

docker push tuliosouzasl2021/kube-news:v1 

se não subir usar o docker login

docket tag tuliosouzasl2021/kube-news:v1 tuliosouzasl2021/kube-news:latest

docker push tuliosouzasl2021/kube-news:latest

criação do manifesto do kubernetes.

mkdir k8s
vim deployment.yaml

obs: essa aplicação utiliza um banco de dados postgres e a aplicação, então precisamos fazer dois deployments, um do postgres e da aplicação, vamos começar com o do postgres.

vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: postgre
spec:
 selector:
  matchLabels:
   app: postgre
 template:
  metada:
   labels:
    app: postgre
  spec:
   containers:
   - name: postgre
     image: postgres:15.0
     ports:
     - containerPort: 5432
     env:
     - name: POSTGRES_DB
       value: "kubenews"
     - name: POSTGRES_USER
       value: "kubenews"
     - name: POSTGRES_PASSWORD
       value: "Pg#123"
---
apiVersion: v1
kind: Service
metada:
 name: postgre
spec:
 selector:
  app: postgre
 ports:
 - port: 5432
 type: CLusterIP

kubectl apply -f deployment.yaml

kubectl get all

kubectl port-forward service/postgre 5432:5432

vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: postgre
spec:
 selector:
  matchLabels:
   app: postgre
 template:
  metada:
   labels:
    app: postgre
  spec:
   containers:
   - name: postgre
     image: postgres:15.0
     ports:
     - containerPort: 5432
     env:
     - name: POSTGRES_DB
       value: "kubenews"
     - name: POSTGRES_USER
       value: "kubenews"
     - name: POSTGRES_PASSWORD
       value: "Pg#123"
---
apiVersion: v1
kind: Service
metada:
 name: postgre
spec:
 selector:
  app: postgre
 ports:
 - port: 5432
 type: CLusterIP
---

apiVersion: apps/v1
kind: Deployment
metadata:
 name: web
spec:
 selector:
  matchLabels:
   app: web
 template:
  metada:
   labels:
    app: web
  spec:
   containers:
   - name: web
     image: tuliosouzasl2021/kube-news:v1
     ports:
     - containerPort: 8080
     env:
     - name: DB_DATABASE 
       value: "kubenews"
     - name: DB_USERNAME
       value: "kubenews"
     - name: DB_PASSWORD
       value: "Pg#123" 
     - name: DB_HOST
       value: "postgre"
---
apiVersion: v1
kind: Service
metada:
 name: web
spec:
 selector:
  app: web
 ports:
 - port: 80
   targetPort: 8080
   nodePort: 30000
 type: NodePort 

kubectl apply -f deployment.yaml

kubectl get pods

aumentamos as replicas da aplicação para 4 e depois para 10

vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: postgre
spec:
 selector:
  matchLabels:
   app: postgre
 template:
  metada:
   labels:
    app: postgre
  spec:
   containers:
   - name: postgre
     image: postgres:15.0
     ports:
     - containerPort: 5432
     env:
     - name: POSTGRES_DB
       value: "kubenews"
     - name: POSTGRES_USER
       value: "kubenews"
     - name: POSTGRES_PASSWORD
       value: "Pg#123"
---
apiVersion: v1
kind: Service
metada:
 name: postgre
spec:
 selector:
  app: postgre
 ports:
 - port: 5432
 type: CLusterIP
---

apiVersion: apps/v1
kind: Deployment
metadata:
 name: web
spec:
 replicas: 10
 selector:
  matchLabels:
   app: web
 template:
  metada:
   labels:
    app: web
  spec:
   containers:
   - name: web
     image: tuliosouzasl2021/kube-news:v1
     ports:
     - containerPort: 8080
     env:
     - name: DB_DATABASE 
       value: "kubenews"
     - name: DB_USERNAME
       value: "kubenews"
     - name: DB_PASSWORD
       value: "Pg#123" 
     - name: DB_HOST
       value: "postgre"
---
apiVersion: v1
kind: Service
metada:
 name: web
spec:
 selector:
  app: web
 ports:
 - port: 80
   targetPort: 8080
   nodePort: 30000
 type: NodePort 


vamos alterar no codigo o nome do navbar e fazer todo o processo de deploy no cluster de kubernetes.

docker build -t tuliosouzasl2021/kube-news:v2 . 
docker build -t tuliosouzasl2021/kube-news:v2 -f Dockerfile

docker push tuliosouzasl2021/kube-news:v2 

vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: postgre
spec:
 selector:
  matchLabels:
   app: postgre
 template:
  metada:
   labels:
    app: postgre
  spec:
   containers:
   - name: postgre
     image: postgres:15.0
     ports:
     - containerPort: 5432
     env:
     - name: POSTGRES_DB
       value: "kubenews"
     - name: POSTGRES_USER
       value: "kubenews"
     - name: POSTGRES_PASSWORD
       value: "Pg#123"
---
apiVersion: v1
kind: Service
metada:
 name: postgre
spec:
 selector:
  app: postgre
 ports:
 - port: 5432
 type: CLusterIP
---

apiVersion: apps/v1
kind: Deployment
metadata:
 name: web
spec:
 replicas: 10
 selector:
  matchLabels:
   app: web
 template:
  metada:
   labels:
    app: web
  spec:
   containers:
   - name: web
     image: tuliosouzasl2021/kube-news:v2
     ports:
     - containerPort: 8080
     env:
     - name: DB_DATABASE 
       value: "kubenews"
     - name: DB_USERNAME
       value: "kubenews"
     - name: DB_PASSWORD
       value: "Pg#123" 
     - name: DB_HOST
       value: "postgre"
---
apiVersion: v1
kind: Service
metada:
 name: web
spec:
 selector:
  app: web
 ports:
 - port: 80
   targetPort: 8080
   nodePort: 30000
 type: NodePort 

kubectl apply -f deployment.yaml








